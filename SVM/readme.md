
## 支持向量机算法
### 快速理解支持向量
一句话表明支持向量机算法的目标：
寻找一个最优分类超平面作为决策曲面，使得正例和反例的隔离边界最大化。

SVM的基本思想：我们可以找到多个可以分类的超平面将数据分开，并且优化时希望所有的点都离超平面远。但是实际上离超平面很远的点已经被正确分类，我们让它离超平面更远并没有意义。反而我们最关心是那些离超平面很近的点，这些点很容易被误分类。如果我们可以让**离超平面比较近的点尽可能的远离超平面**，那么我们的分类效果会好有一些。

![](支持向量机(2).png)

**为什么叫支持向量**：在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。


### 算法原理
支持向量机的算法原理过于复杂，不列出具体推导公式，对于数学原理感兴趣的可详细阅读本文中的参考文献。  

总体上来说，支持向量机的凸优化问题，是通过拉格朗日函数将有约束的优化目标转化为无约束的优化函数。

**对偶问题**：具体求解通过原问题的对偶问题求解，也就是原问题的极小极大问题，通过求解对偶问题的极大极小问题解决。  
对偶形式的解与原始解相等需要什么前提条件呢？    
凸函数 && 满足KKT 条件

**软间隔和硬间隔**：

硬间隔最大化的条件:

$\min\frac{1}{2}\left\|\omega\right \|{^2}$   $\quad s.t\quad y_{i}(\omega^Tx_{i}+b)\ge1\quad (i=1,2,...,m)$ 

一般情况下数据中存在异常点导致数据集不能线性可分，这时候引入软间隔最大化，软间隔最大化的条件如下：  

$\min\frac{1}{2}\left\|\omega\right \|{^2}+C\sum_{i=1}^m \xi$ $\quad s.t\quad y_{i}(\omega^Tx_{i}+b)\ge1-\xi_{i}\quad (i=1,2,...,m) \quad \xi\ge0\quad (i=1,2,...,m)$ ,  

SVM对训练集里面的每个样本$(xi,yi)$引入了一个松弛变量$\xi_{i}$,使函数间隔加上松弛变量大于等于1即可。对比硬间隔最大化，可以看到我们对样本到超平面的函数距离的要求放松了，之前是一定要大于等于1，现在只需要加上一个大于等于0的松弛变量能大于等于1就可以了。当然，松弛变量不能白加，这是有成本的，每一个松弛变量$\xi_{i}$, 对应了一个代价$\xi_{i}$。这里,$C>0$为惩罚参数，可以理解为我们一般回归和分类问题正则化时候的参数。C越大，对误分类的惩罚越大，C越小，对误分类的惩罚越小。

**核技巧**：  
对于在低维线性不可分的数据，在映射到了高维以后，就变成线性可分的了。这个思想我们同样可以运用到SVM的线性不可分数据上。也就是说，对于SVM线性不可分的低维特征数据，我们可以将其映射到高维，就能线性可分，此时就可以运用线性可分SVM的算法思想了。  
引入核函数的过程：我们遇到线性不可分的样例时，常用做法是把样例特征映射到高维空间中去，但是遇到线性不可分的样例，一律映射到高维空间，那么这个**维度大小是会高到令人恐怖的**。此时，核函数就体现出它的价值了，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数好在它在低维上进行计算，而将实质上的分类效果（利用了内积）表现在了高维上，这样避免了直接在高维空间中的复杂计算，真正解决了SVM线性不可分的问题。  

**SMO算法求解**：  
SVM有很多种实现，最流行的一种实现是： 序列最小优化(Sequential Minimal Optimization, SMO)算法。  
SMO思想：是将大优化问题分解为多个小优化问题来求解的。


### 代码实现
当给定训练集时，我们无非就是先计算出所有的先验概率和条件概率，然后把它们存起来（当成一个查找表）。当来一个测试样本时，我们就计算它所有可能的后验概率，最大的那个对应的就是测试样本的类别，而后验概率的计算无非就是在查找表里查找需要的值。  

[SVM的Sklearn实现](svm_sklearn.py)

[SVM的Python实现](svm_complete_Non-Kernel.py)

### 参考文献
[支持向量机原理系列](https://www.cnblogs.com/pinard/p/6097604.html)  

