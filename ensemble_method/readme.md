
## 集成学习ensemble
### 快速理解集成学习
**概念**：集成学习并不是一种具体的机器学习模型，而是一种技术框架，是按照不同的思路来组合基学习模型，从而达到提升模型准确率的效果。背后的思想就是“三个臭皮匠赛过诸葛亮”，如果要使得集成学习的效果越好，就要求每个基学习器的模型准确率越高，同时各个基学习间也要保持多样性。

#### 集成学习之结合策略
##### 平均法
对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。

##### 投票法
对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是{c1,c2,...cK},对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是(h1(x),h2(x)...hT(x))。

最简单的投票法是相对多数投票法，也就是我们常说的少数服从多数，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。

稍微复杂的投票法是绝对多数投票法，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。

更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。
##### 学习法
上两节的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时，我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。

在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。
#### 主流的集成学习方法
目前常见的集成学习框架：bagging，boosting。基于Boosting的代表算法有Adaboost、GBDT、XGBOOST、LightGBM；基于Bagging的代表算法主要是随机森林。下面将对每个框架逐一进行介绍。

### Boosting
Boosting就是提升的意思，就是从弱学习算法出发，得到一系列弱分类器,然后组合这些弱分类器，最终得到一个强分类器。Boosting是前后的两个弱分类器间存在依赖关系，必须串行生成的方法。顺序运行会导致运行速度慢。其中最具代表性的算法为Adaboost算法。

#### Adaboosting
Adaboost算法整体思路：先赋予初始训练集相同的权重，然后训练出一个基学习器，再根据基学习器的分类结果对训练集的权重进行调整，**使得之前基学习器分类错误的样本在后续学习过程中受到更多的关注，也就是加大它的权重**,然后再根据调整后的权重进行下一轮的学习;如此反复，直到达到规定的次数。**这里涉及到两个权重，每轮新训练的预测函数在最终预测函数中所占的权重和样本下一轮训练中的权重。**

Adaboost算法的关键步骤：  
1. 初始化训练集的权重值,按照训练集的个数进行权值分配  
$D_{1}=(w_{11},,,w_{1i},,,w_{1N})$ ,  $\quad w_{1i}=\frac{1}{N}$,    $\quad i=1,2,,,N$
2. 对$m=1,2,,,M$,也就是一共需要多少个基分类器  
**(a).** 使用具有权值分布为$D_{m}$的训练数据集学习，得到基本分类器$G_{m}(x)$  
**(b).** 计算$G_{m}(x)$在训练数据集上的分类误差,也就是对误分类的样本权重求和  
$e_{m}=P(G_{m}(x)\neq y_{i})=\sum_{i=1}^Nw_{mi}I(G_{m}(x)\neq y_{i})$  
**(c).** 计算$G_{m}(x)$的系数，这个系数是作为当前基分类器最终加权时候的系数，表示 $G_{m}(x)$在最终分类器中的重要性，也就是分类误差越小的基分类器在最终分类器中的作用和影响越大。  
$\alpha_{m}=\frac{1}{2}log\frac{1-e_{m}}{e_{m}}$  
**(d).** 更新训练集权值分布，由下式可知，被基本分类器$G_{m}(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。  
$D_{m+1}=(w_{m+1,1},,,w_{m+1,i},,,w_{m+1,N})$  
$w_{m+1,i}=\frac{w_{m,i}}{Z_{m}}\exp(-\alpha_{m}y_{i}G_{m}(x_{i}))\quad i=1,2,...,N$  
这里Z_{m}是规范化因子  
$Z_{m}=\sum_{i=1}^Nw_{m,i}\exp(-\alpha_{m}y_{i}G_{m}(x_{i}))$  
3. 构建基本分类器的线性组合：  
$f(x)=\sum_{m=1}^M\alpha_{m}G_{m}(x)$  
得到最终分类器：  
$G(x) = sign(f(x)) = sign(\sum_{m=1}^M\alpha_{m}G_{m}(x))$

另外一点需要注意的是，**Adaboost的训练误差是以指数速率下降的。**这个性质告诉我们，Adaboost能在学习过程中不断减少训练误差。

#### 提升树（Boosted Tree有各种马甲，比如GBDT, GBRT (gradient boosted regression tree)）
提升树是以分类树或回归树为基本分类器的提升方法。提升树模型采用加法模型（基函数的线性组合）与前向分步算法，同时基函数采用决策树算法，对待分类问题采用二叉分类树，对于回归问题采用二叉回归树。

其中对于二分类问题，提升树算法与前面提到的Adaboost算法思路雷同，只是将Adaboost算法中的基本分类器限制为二类分类树即可，可以说这时的提升树算法是Adaboost算法的特殊情况。这里重点讲一下回归提升树。

回归问题提升树仍然使用以下的前向分布算法：  
$f(x_{0})=0$  
$f_{m}(x)=f_{m-1}(x)+T(x;\theta_{m})$,其中$\theta_{m}$为决策树的参数  
$f_{M}(x) = \sum_{m=1}^M(x;\theta_{m})$  
在前向分布算法的第m步，给定当前模型$f_{m-1}(x)$,需求解$\theta_{m}=arg\min_{\theta_{m}}\sum_{i=1}^NL(y_{i},f_{m-1}(x)+T(x_{i};\theta_{m})$,得到第m棵树的参数。

当采用平方误差损失函数时，  
$L(y,f(x))=(y-f(x))^2$  
其损失变为：  
$L(y,f_{m-1}(x)+T(x_{i};\theta_{m}))$  
$\quad \quad =[y-f_{m-1}(x)-T(x_{i};\theta_{m})]^2$  
$\quad \quad =[r-T(x_{i};\theta_{m})]^2$   

这里$r=y-f_{m-1}(x)$,**是当前模型拟合数据的残差**，所以对于回归问题的提升树来说，只需要简单地拟合当前模型的残差。

##### GBDT
###### GBDT实现基本思路
GBDT是通过采用加法模型（即基函数的线性组合），以及**不断减小训练过程产生的残差**来达到将数据分类或者回归的算法。GBDT的弱分类器一般会选择为CART TREE（也就是分类回归树）。

我们真正关注的，1.是希望损失函数能够不断的减小，2.是希望损失函数能够尽可能快的减小。所以如何尽可能快的减小呢？

这就提到了GBDT的核心，利用**损失函数的负梯度在当前模型的值**作为回归问题提升树算法中的残差的近似值去拟合一个回归树。GBDT每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度。这样每轮训练的时候都能够让损失函数尽可能快的减小，尽快的收敛达到局部最优解或者全局最优解。

Gradient Boost与传统的Boost的区别是，Gradient Boost会定义一个loss Function，每一次的计算是为了减少上一次的loss，而为了消除loss，可以在loss减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的loss往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。
 
###### GBDT回归算法
@import "GBDT回归.png"

###### GBDT分类算法
GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。

###### GBDT调参
在scikit-learn中，GradientBoostingClassifier为GBDT的分类库， 而GradientBoostingRegressor为GBDT的回归库。两者的参数类型完全相同，当然有些参数比如损失函数loss的可选择项并不相同。
这些参数中，重要参数分为两类，**第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数**。

一、GBDT类库boosting框架参数
首先，我们来看boosting框架相关的重要参数。由于GradientBoostingClassifier和GradientBoostingRegressor的参数绝大部分相同，我们下面会一起来讲，不同点会单独指出。

1) n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。

2) learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为fk(x)=fk−1(x)+νhk(x)。ν的取值范围为0<ν≤1。对于同样的训练集拟合效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的ν开始调参，默认是1。

3) subsample: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。

4) init: 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的f0(x)，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。

5) loss: 即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。

　　对于分类模型，有对数似然损失函数"deviance"和指数损失函数"exponential"两者输入选择。默认是对数似然损失函数"deviance"。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的"deviance"。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。

　　对于回归模型，有均方差"ls", 绝对损失"lad", Huber损失"huber"和分位数损失“quantile”。默认是均方差"ls"。一般来说，如果数据的噪音点不多，用默认的均方差"ls"比较好。如果是噪音点较多，则推荐用抗噪音的损失函数"huber"。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。

6) alpha：这个参数只有GradientBoostingRegressor有，当我们使用Huber损失"huber"和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。


二、GBDT树相关的参数
这里我们再对GBDT的类库弱学习器的重要参数做一个总结。由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。

1) 划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑log2N个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑N−−√个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。

2) 决策树最大深度max_depth: 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。

3) 内部节点再划分所需最小样本数min_samples_split: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

4) 叶子节点最少样本数min_samples_leaf: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值
5) 叶子节点最小的样本权重和min_weight_fraction_leaf：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

6) 最大叶子节点数max_leaf_nodes: 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

7) 节点划分最小不纯度min_impurity_split:  这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。

###### GBDT的优点
GBDT受欢迎的几个关键因素，一是模型效果表现不错；二是既可以用于分类也可以用于回归；三是可以筛选特征； 

##### XGboost
XGboost算法也是一种boosting算法，前后的两个弱分类器间存在依赖关系，第t次迭代的代价函数里包含了前面t-1次迭代的预测值。XGboost因为在各类数据竞赛中的良好表现而引起越来越多的关注，另外它的处理效率和速度也都有很好的表现。

###### 算法概述
XGboost的目标函数如下：
$Obj=\sum_{i=1}^nl(y_i,\hat{y_i})+\sum_{k=1}^K\Omega(f_k)$
前面部分对应着预测误差，误差越小越好；后面为正则项；其中正则项控制着模型的复杂度，包括了叶子节点数目T和叶子节点取值的L2模的平方,相当于在训练过程中做了剪枝：
$\Omega(f_k)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^T\omega_j^2$
目标要求**预测误差尽量小，叶子节点尽量少，节点取值尽量不极端**

上述目标函数要考虑的两个关键因素，第一选取哪个feature作为分裂节点；第二，节点的预测值。
XGboost是通过**贪心策略+二次最优化**来解决上述问题的。
贪心策略用于选择分裂点，采取的是当前利益最大化原则，选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值，当枚举完全部feature时，找一个效果最好的，把树给分裂；在XGboost中通过并行化的方法，加快了分裂点选取的计算过程，效率大大提高。

二次最优化指的是，对损失函数进行了二阶泰勒展开，同时用到了一阶和二阶导数；其中节点的预测值$\omega$，是将损失函数看成关于$\omega$的二次函数，通过求损失函数的最小值，取最小值的点就是这个节点的预测值$\omega$。

###### xgboost相比传统gbdt有何不同？xgboost为什么快？xgboost如何支持并行？(引自文献2)
- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
- 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
- xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

###### XGboost调参
XGBoost的作者把所有的参数分成了三类：
- 通用参数：宏观函数控制
- Booster参数：控制每一步的booster(tree/regression)
- 学习目标参数：控制训练目标的表现

一、通用参数
这些参数用来控制XGBoost的宏观功能。
1、booster[默认gbtree]
选择每次迭代的模型，有两种选择： 
gbtree：基于树的模型 
gbliner：线性模型
2、silent[默认0]
当这个参数值为1时，静默模式开启，不会输出任何信息。
一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。
3、nthread[默认值为最大可能的线程数]
这个参数用来进行多线程控制，应当输入系统的核数。
如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。
还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。

二、booster参数
尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。
1、eta[默认0.3]
和GBM中的 learning rate 参数类似。
通过减少每一步的权重，可以提高模型的鲁棒性。
典型值为0.01-0.2。
2、min_child_weight[默认1]
决定最小叶子节点样本权重和。
和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。
这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。
但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。
3、max_depth[默认6]
和GBM中的参数相同，这个值为树的最大深度。
这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。
需要使用CV函数来进行调优。
典型值：3-10
4、max_leaf_nodes
树上最大的节点或叶子的数量。
可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成$n^2$个叶子。
如果定义了这个参数，GBM会忽略max_depth参数。
5、gamma[默认0]
在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。
这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。
6、max_delta_step[默认0]
这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。
通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。
这个参数一般用不到，但是你可以挖掘出来它更多的用处。
7、subsample[默认1]
和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。
减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。
典型值：0.5-1
8、colsample_bytree[默认1]
和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。
典型值：0.5-1
9、colsample_bylevel[默认1]
用来控制树的每一级的每一次分裂，对列数的采样的占比。
我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。
10、lambda[默认1]
权重的L2正则化项。(和Ridge regression类似)。
这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。
11、alpha[默认1]
权重的L1正则化项。(和Lasso regression类似)。
可以应用在很高维度的情况下，使得算法的速度更快。
12、scale_pos_weight[默认1]
在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。

三、学习目标参数
这个参数用来控制理想的优化目标和每一步结果的度量方法。
1、objective[默认reg:linear]
这个参数定义需要被最小化的损失函数。最常用的值有： 
binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。
multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 
在这种情况下，你还需要多设一个参数：num_class(类别数目)。
multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。
2、eval_metric[默认值取决于objective参数的取值]
对于有效数据的度量方法。
对于回归问题，默认值是rmse，对于分类问题，默认值是error。
典型值有： 
rmse 均方根误差
mae 平均绝对误差
logloss 负对数似然函数值
error 二分类错误率(阈值为0.5)
merror 多分类错误率
mlogloss 多分类logloss损失函数
auc 曲线下面积
3、seed(默认0)
随机数的种子
设置它可以复现随机数据的结果，也可以用于调整参数

**参数调优的一般方法**,需要进行如下步骤：
- 选择较高的学习速率(learning rate)。学习效率和迭代次数彼此约束，共同确定，先选择一个较高的学习率。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。然后选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。
- 对于给定的学习速率和决策树数量，进行决策树特定参数调优。(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数。
- xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。
- 然后降低第一步中初始设置的学习速率，确定理想参数。
python的具体例子见文献6.

##### LightGBM
LightGBM是微软提出的梯度boosting框架，通过名字可以看出来，这个框架是light即轻量级的，相较于另外两个知名的boosting框架GBDT和XGBoost，LightGBM具有如下优势：
- 更快的训练效率
- 低内存使用
- 更高的准确率

LightGBM针对于XGBoost的缺点，提出了如下改进：
- LightGBM采用Histogram算法，其思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram。然后遍历训练数据，统计每个离散值在直方图中的累计统计量。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点
- LightGBM的leaf-wise的生长策略，它摒弃了现在大部分GBDT使用的按层生长（level-wise）的决策树生长策略，使用带有深度限制的按叶子生长（leaf-wise）的策略。level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。 
###### LightGBM调参
LightGBM相关的参数较多，详细的参数说明这里不罗列，具体可见文献7的官方文档。

其中针对于LightGBM的技术特点，调参时重点关注如下几个方面的内容：
一、针对 Leaf-wise (最佳优先) 树的参数优化
LightGBM 使用 leaf-wise 的树生长策略, 而很多其他流行的算法采用 depth-wise 的树生长策略. 与 depth-wise 的树生长策略相较, leaf-wise 算法可以收敛的更快. 但是, 如果参数选择不当的话, leaf-wise 算法有可能导致过拟合。想要在使用 leaf-wise 算法时得到好的结果, 这里有几个重要的参数值得注意:
1. num_leaves. 这是控制树模型复杂性的重要参数。理论上，我们可以通过设定num_leaves = 2^(max_depth) 去转变成为depth-wise tree。但这样容易过拟合，因为当这两个参数相等时,  leaf-wise tree的深度要远超depth-wise tree。因此在调参时，往往会把 num_leaves的值设置得小于2^(max_depth)。例如当max_depth=6时depth-wise tree可以有个好的准确率，但如果把 num_leaves 设成 127 会导致过拟合，要是把这个参数设置成 70或 80 却有可能获得比depth-wise tree有更好的准确率。事实上，当我们用 leaf-wise tree时，我们可以忽略depth这个概念，毕竟leaves跟depth之间没有一个确切的关系。

2. min_data_in_leaf. 这是另一个避免leaf-wise tree算法过拟合的重要参数。该值受到训练集数量和num_leaves这两个值的影响。把该参数设的更大能够避免生长出过深的树，但也要避免欠拟合。在分析大型数据集时，该值区间在数百到数千之间较为合适。

3. max_depth. 也可以通过设定 max_depth 的值来限制树算法生长过深。


### Bagging

#### Bagging的策略
1. 给定一个大小为d训练集D;
2. Bagging通过从D中进行可重复的均匀采样 （by sampling from D uniformly and with replacement），得到m个新的训练集Di, 每个Di的大小为n ;
3. 将得到的m个新训练集，分别进行回归或者分类，得到m个最终的结果；
4. 由m个结果得到最终结果，对于回归：将m个结果进行平均；对于分类：将m个结果进行投票，票数多的为最终分类值。

#### 随机森林
随机森林(Random Forest,以下简称RF)是Bagging算法的进化版，也就是说，它的思想仍然是bagging,但是进行了独有的改进。我们现在就来看看RF算法改进了什么。　　　

首先，RF使用了CART决策树作为弱学习器，这让我们想到了梯度提升树GBDT。第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为$nsub$，然后在这些随机选择的$nsub$个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　　　　

如果nsub=n，则此时RF的CART决策树和普通的CART决策树没有区别。nsub越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说nsub越小，模型的方差会减小，但是偏差会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的nsub的值。

除了上面两点，RF和普通的bagging算法没有什么不同， 下面简单总结下RF的算法。
输入为样本集D={(x,y1),(x2,y2),...(xm,ym)}，弱分类器迭代次数T。
输出为最终的强分类器f(x)
1)对于t=1,2...,T:
  a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dt
  b)用采样集Dt训练第t个决策树模型Gt(x)，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分
2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

**Bagging与Boosting二者的区别**：
  (1)从重采样的角度：Bagging对样本进行重采样；相对的，Boosting重采样的不是样本，而是样本分布，对正确分类的样本降权，对错误分类的样本加权（而这些样本通常就是分类边界附近的样本），最后分类器是多个弱分类器的加权组合（线性叠加）， AdaBoost是其中代表方法。
  (2)从最终目标角度：Bagging 可以看作是提升不稳定分类器的效果；而Boosting 最终目标是提高弱分类器的分类精度。
  (3)Bagging旨在减少方差，而boosting旨在减少偏差。


### Stacking
Stacked generalization (stacking) (Wolpert, 1992) 是一种组合多个模型的方法，虽然是一个不错的idea，但是实际应用中不如bagging和boosting广泛。Stacking常用来组合不同类型的模型。 

Stacking的步骤如下：
1. 将训练集分成两个不相交的部分；
2. 在第一部分的训练集上训练若干个基本学习器；
3. 在第二部分的训练集上测试得到的基本学习器；
4. 使用步骤3中的预测结果作为输入，将正确的响应（responses）作为输出，训练更高级别的学习器。
步骤1到3，类似与交叉验证（cross-validation），但是不同于winner-takes-all的策略，stacking通过组合基本分类器来得到更高级的学习结果。

### 代码实现


[GBDT算法实现以及调参实例](gbdt_sklearn.py)

[XGBoost算法实现以及调参实例](xgboost_parameter_tuning.py)

### 参考文献
[文献1：梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)  
[文献2：XGBoost浅入浅出](http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/)
[文献3：通俗、有逻辑的写一篇说下Xgboost的原理，供讨论参考](https://blog.csdn.net/github_38414650/article/details/76061893)
[文献4：具体数学公式推导可见：大神陈天奇论文](http://www.52cs.org/?p=429)
[文献5：scikit-learn 梯度提升树(GBDT)调参小结](http://www.cnblogs.com/pinard/p/6143927.html)
[文献6：Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
[文献7：LightGBM官方中文文档](http://lightgbm.apachecn.org/cn/latest/Parameters.html)
[文献8：集成学习原理小结](http://www.cnblogs.com/pinard/p/6131423.html)
[文献9：Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)