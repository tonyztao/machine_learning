
## 朴素贝叶斯算法
### 一些基本概念

先验概率:是指根据以往经验和分析得到的概率。
意思是说我们人有一个常识,比如骰子,我们都知道概率是1/6,而且无数次重复实验也表明是这个数,这是一种我们人的常识,也是我们在不知道任何情况下必然会说出的一个值。而所谓的先验概率是我们人在未知条件下对事件发生可能性猜测的数学表示

后验概率：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，简短理解就是知果求因。

### 朴素贝叶斯分类算法
朴素贝叶斯方法的简短解释：朴素：特征条件独立；贝叶斯：基于贝叶斯定理；贝叶斯定理如下：$P(B|A) = \frac{P(A|B)P(B)}{P(A)}$

上面的公式不易于理解，换个表达形式就会明朗很多，如下所示：
$P(class|feature)=\frac{P(feature|class)P(class)}{P(feature)}$  

故对于一个分类问题，就是在给定样本特征时候，计算样本属于各个类别的概率，取最大的概率值为所属分类。

#### 原理与流程
朴素贝叶斯方法进行分类的流程如下：
1. 设$x = \left.\{ a_{1},a_{2},...,a_{m}\right.\}$为一个待分类项，而每个$a$为$x$的一个特征属性。
2. 有类别集合$C = \left.\{ y_{1},y_{2},...,y_{n}\right.\}$
3. 计算$P(y_{1}|x),P(y_{2}|x),...,P(y_{n}|x)$
4. 如果$P(y_{k}|x) = max(P(y_{1}|x),P(y_{2}|x),...,P(y_{n}|x))$,则$x \in y_{k}$

现在的关键就是如何计算第3步中的各个条件概率。可以这么做：
1. 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。
2. 统计得到在各类别下各个特征属性的条件概率估计。即  
$P(a_{1}|y_{1}),P(a_{2}|y_{1}),...,P(a_{m}|y_{1});P(a_{1}|y_{2}),P(a_{2}|y_{2}),...,P(a_{m}|y_{2});...;P(a_{1}|y_{n}),P(a_{2}|y_{n}),...,P(a_{m}|y_{n})$
3. 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：  $P(y_{i}|x)=\frac{P(x|y_{i})P(y_{i})}{P(x)}$  
因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：  
$P(x|y_{i})P(y_{i})=P(a_{1}|y_{i})P(a_{2}|y_{i})...P(a_{m}|y_{i})P(y_{i}) = P(y_{i})\prod\limits_{j=1}^{m}P(a_{j}|y_{i})$

### 拉普拉斯平滑
为什么要做平滑处理?  
零概率问题，就是在计算实例的概率时，如果某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0。  
**解决方法：**
在计算概率公式时，分子和分母都分别加上一个常数，就可以避免这个问题。当加入的λ=1时则避免了0概率问题，这种方法被称为拉普拉斯平滑。
引入Laplace校准，它的思想非常简单，就是对每类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。

### 连续变量处理
**高斯模型**：当特征是连续变量的时候，运用多项式模型就会导致很多P(xi|yk)=0（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。  
高斯模型假设每一维特征都服从高斯分布（正态分布），通过一个例子说明：  
已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？ 根据朴素贝叶斯分类器，计算下面这个式子的值：  
P(身高|性别) x P(体重|性别) x P(脚掌|性别) x P(性别)  
这里的困难在于，由于身高、体重、脚掌都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办？ 
这时，可以假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。 

### 朴素贝叶斯算法的优缺点：
朴素贝叶斯的主要优点有：

1）朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。

2）对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。

3）对缺失数据不太敏感，算法也比较简单，常用于文本分类。

朴素贝叶斯的主要缺点有：　　　

1） 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

2）需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。

3）由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。

4）对输入数据的表达形式很敏感。
### 朴素贝叶斯算法的注意点：
- 大家也知道，很多特征是连续数值型的，但是它们不一定服从正态分布，一定要想办法把它们变换调整成满足正态分布！！
- 对测试数据中的0频次项，一定要记得平滑，简单一点可以用『拉普拉斯平滑』。
- 先处理处理特征，把相关特征去掉，因为高相关度的2个特征在模型中相当于发挥了2次作用。
- 朴素贝叶斯分类器一般可调参数比较少，比如scikit-learn中的朴素贝叶斯只有拉普拉斯平滑因子alpha，类别先验概率class_prior和预算数据类别先验fit_prior。模型端可做的事情不如其他模型多，因此我们还是集中精力进行数据的预处理，以及特征的选择吧。
- 一般其他的模型(像logistic regression，SVM等)做完之后，我们都可以尝试一下bagging和boosting等融合增强方法。咳咳，很可惜，对朴素贝叶斯里这些方法都没啥用。原因？原因是这些融合方法本质上是减少过拟合，减少variance的。朴素贝叶斯是没有variance可以减小。

### 代码实现
当给定训练集时，我们无非就是先计算出所有的先验概率和条件概率，然后把它们存起来（当成一个查找表）。当来一个测试样本时，我们就计算它所有可能的后验概率，最大的那个对应的就是测试样本的类别，而后验概率的计算无非就是在查找表里查找需要的值。  

[Naive Bayes的Sklearn实现](https://github.com/tonyztao/machine_learning/blob/master/Naive_Bayes/Naive_Bayes_Sklearn_Iris_Classification.py)

[Naive Bayes的Python实现](https://github.com/tonyztao/machine_learning/blob/master/Naive_Bayes/Naive_Bayes.py)

[Naive Bayes的Kaggle实例](https://github.com/tonyztao/machine_learning/blob/master/Naive_Bayes/Naive_Bayes_Kaggle.py)

### 参考文献
[算法杂货铺——分类算法之朴素贝叶斯分类](http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html)  
[朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)  
[朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)
[朴素贝叶斯实战与进阶](http://blog.csdn.net/han_xiaoyang/article/details/50629608)

