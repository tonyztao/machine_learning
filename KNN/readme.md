
## K近邻
#### K近邻的基本思路
KNN算法的思想总结：在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：
- 计算测试数据与各个训练数据之间的距离；
- 按照距离的递增关系进行排序；
- 选取距离最小的K个点；
- 确定前K个点所在类别的出现频率；
- 返回前K个点中出现频率最高的类别作为测试数据的预测分类。


#### k近邻算法的优缺点：
1. 简单，易于理解，易于实现，无需估计参数，无需训练；
2. k近邻算法具有精度高，对异常值不敏感的优点；
3. 适合对稀有事件进行分类；
4. 特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好。
5. K近邻算法为lazy learning懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢；
6. 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数；

#### K近邻的三个关键因素

三个关键因素：k的取值、距离的度量、类别的归属规则（分类表决）；
##### k值选定
k值选择过小，得到的近邻数过少，会降低分类精度，同时也会放大噪声数据的干扰；而如果k值选择过大，并且待分类样本属于训练集中包含数据数较少的类，那么在选择k个近邻的时候，实际上并不相似的数据亦被包含进来，造成噪声增加而导致分类效果的降低。

k值通常是采用交叉检验来确定（以k=1为基准），从K=1开始，逐个计算预测准确率，取准确率最高时的K值。K值一般低于训练样本数的平方根。参数k的取值一般通常不大于20。——《机器学习实战》
##### 距离度量
计算所有训练样本与待分类样本的距离，距离近的为邻居。计算距离的方法有很多：欧式距离、余弦距离、Jaccard系数、皮尔森相关系数等。

##### 类别归属判定
1.投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。（上图中，就采用这种方法）。

2.加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近权重越大（权重为距离平方的倒数）。


#### K近邻的实现方法 —— kd树

#### 代码实现
[KNN的Sklearn实现](https://github.com/tonyztao/machine_learning/blob/master/KNN/knn_sklearn.py)

[KNN的Python实现](https://github.com/tonyztao/machine_learning/blob/master/KNN/knn.py)

#### 参考文献
[机器学习实战之kNN算法](https://www.cnblogs.com/zy230530/p/6780836.html) 


